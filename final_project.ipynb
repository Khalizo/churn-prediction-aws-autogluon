{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24712991-b561-4bfe-a2b8-7425354f520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, precision_recall_curve, roc_curve\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import timeit\n",
    "import pprint\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.callbacks import DeadlineStopper, DeltaYStopper\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb287a6e-80ea-4554-bf18-1d2028a1c68b",
   "metadata": {},
   "source": [
    "# Sections of the report \n",
    "\n",
    "-\tIntroduction\n",
    "-\tMethods\n",
    "    -\tCleaning the data and creating new input features\n",
    "    - Analysing and visualising the data\n",
    "    - Preparing the inputs and choosing suitable features\n",
    "    - Selecting and training a model\n",
    "-\tEvaluation\n",
    "\n",
    "-\tConclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b92da9-bdc3-4d21-a83b-7b8f1f40069e",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The telecommunications sector has become one of the main industries in developed countries. The technical progress and the increasing number of operators globally have made the industry competititive. Companies are working hard to survive in this competitive market depending on multiple strategies.\n",
    "\n",
    "There are often three main strategies for generating more revenue within a business: \n",
    "1) Acquiring new customers\n",
    "2) Upselling existing customers \n",
    "3) Increase the retention period of customers\n",
    "\n",
    "However, comparing these strategies taking the value of return on investment (RoI) of each into account has shown that the third strategy is the most profitable strategy. \n",
    "The reason being is that retaining an existing customer costs much lower than acquiring a new one, in addition to being considered much easier than the upselling strategy. \n",
    "To apply the third strategy, we need to decrease the potential of customer churn by putting systems in place to do so. Hence why exploring machine learning techniques for predicting customer churn can provide huge financial benefits to companies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d261d2-6a50-48f5-9a5e-b11af0c64fd7",
   "metadata": {},
   "source": [
    "## 2. Method\n",
    "\n",
    "### 2.1 Loading and cleaning the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad17b6ba-f350-4548-9a18-708cebfb8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train = pd.read_csv('proposal_docs/dataset/train.csv', index_col= False)\n",
    "test = pd.read_csv('proposal_docs/dataset/test.csv', index_col= False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce641d0-d680-46e7-ab23-7125b3c05654",
   "metadata": {},
   "source": [
    "### Cleaning the dataset \n",
    "\n",
    "a) **Check for Null-values and inconsistent data types:** Upon looking at that dataset, we can see that each column has an equal amount of non-null values, indicating that there are **no instances** of missing data. \n",
    "\n",
    "However, the datatypes in the datasets are of type objet, int64, float64 indicating varying data types and a mixture between categorical and numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01d1831-6722-4fcc-9ae7-e7af0389d02e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for Null-values\n",
    "print(train.info(null_counts=True))\n",
    "print(test.info(null_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e3f04d-c48d-46d1-b1e2-c697d186dd0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe511e8-bf92-4ca2-a10e-dc6978027c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a1b38-b629-4f96-b117-fe99d572f46b",
   "metadata": {},
   "source": [
    "b) **Check for duplicate values**: No duplicate values were found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853ef9d2-6533-4ee4-ad2e-34607cb3bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [train, test]\n",
    "\n",
    "for data in datasets: \n",
    "    duplicate_rows = data[data.duplicated()]\n",
    "    print(len(duplicate_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3351995d-95c0-439b-8f0b-9ca740df72da",
   "metadata": {},
   "source": [
    "### 2.1.2 Create new input features\n",
    "- Created additional features `total_minutes`, `total_calls`  and `total_charges` to obsverve correlations and and see if that affects the model as well\n",
    "- One-hot encoded the categorical features to in order to create a correlation matrix of all features\n",
    "- label encoded the target variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e119e586-ce0c-46c2-9503-e6bb12a63b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_total_minutes_calls_charge(dataset):\n",
    "    \"\"\"\n",
    "    Function for totally the minutes, calls and charges features\n",
    "    :dataset: churn X_train or X_test dataset\n",
    "    \"\"\"\n",
    "    dataset['total_minutes'] = dataset['total_day_minutes']  + dataset['total_eve_minutes'] + dataset['total_night_minutes']+ dataset['total_intl_minutes']\n",
    "    dataset['total_calls'] = dataset['total_day_calls'] + dataset['total_eve_calls'] + dataset['total_night_calls'] + dataset['total_intl_calls']\n",
    "    dataset['total_charge'] = dataset['total_day_charge'] + dataset['total_eve_charge'] + dataset['total_night_charge'] + dataset['total_intl_charge']\n",
    "    return dataset\n",
    "\n",
    "train = add_total_minutes_calls_charge(train)\n",
    "test  = add_total_minutes_calls_charge(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa9611-49e3-47b5-ac42-42c2e9a6e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Y_train to 0 and 1 \n",
    "\n",
    "le = LabelEncoder()\n",
    "train['churn'] = le.fit_transform(train['churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76423ee4-854c-41b4-a222-6a7b025dcbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categorical variables to 1 hot encoding\n",
    "cols = train.columns\n",
    "feature_cols = list(train.columns)\n",
    "feature_cols.remove('churn')\n",
    "\n",
    "# Get the numerical and categorical columns\n",
    "num_cols = train._get_numeric_data().columns\n",
    "cat_cols = list(set(cols) - set(num_cols))\n",
    "\n",
    "#one hot encode the testing and training set\n",
    "train_one_hot = pd.get_dummies(data = train, columns = cat_cols)\n",
    "test_one_hot = pd.get_dummies(data = test, columns = cat_cols)\n",
    "\n",
    "# Get feature cols\n",
    "feature_cols = [x for x in train_one_hot.columns if x != 'churn' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b7509b-62ec-4f93-88b1-0bb54271818f",
   "metadata": {},
   "source": [
    "### 2.1.3 Create a validation set\n",
    "\n",
    "Despite there being a already a test set in place, a preliminary validation set was created, to evaluate the performance of the model and to see how well the model could\n",
    "generalise on unseen data. The training data was split with an 90/10 split in stratified fashion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a5450-2b64-4063-a909-ac39b1784c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y datasets for machine learning purposes\n",
    "X_train, y_train, X_test = train_one_hot[feature_cols], train['churn'], test_one_hot[feature_cols]\n",
    "\n",
    "# Split the training set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train, random_state=27)\n",
    "\n",
    "# Resets the indexes\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efb8eec-328a-4759-adf9-cf8fcda639df",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Analysing and Visualising the dataset\n",
    "\n",
    "#### 2.2.1 Visualising the class distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc3851-93f4-411b-b093-93f3a61c9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_value_counts = y_train.value_counts()\n",
    "classes = np.array(class_value_counts.keys())\n",
    "class_freq = class_value_counts.values\n",
    "title = 'Churn'\n",
    "\n",
    "def get_pct(array, value):\n",
    "    \"\"\"\n",
    "    Gets the percentage of a value in proportion to the sum of an array\n",
    "    return: pct_string\n",
    "    \"\"\"\n",
    "    pct = value/array.sum()\n",
    "    pct_string =  \" ({:.2%})\".format(pct)\n",
    "    return pct_string\n",
    "#\n",
    "def plot_class_dist(data,classes, title):\n",
    "    \"\"\"\n",
    "    Plots a 2-D plot, showing the relationship between classes and frequency\n",
    "    Plots the images seen in Figure 2 of the report. \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))    \n",
    "    width = 0.75 # the width of the bars \n",
    "    ind = np.arange(len(data))  # the x locations for the groups\n",
    "    ax.barh(ind, data, width, color=\"blue\")\n",
    "    ax.set_yticks(ind+width/2)\n",
    "    ax.set_yticklabels(classes, minor=False)\n",
    "    plt.title(\"Class Frequency Distribution of the \" + title +  \" Dataset\")\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Classes')\n",
    "    for i, v in enumerate(data):\n",
    "        ax.text(v + 3, i + .25, str(v) + get_pct(data,v), color='green', fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "plot_class_dist(class_freq, classes, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b9322-235c-473d-a065-be1a4824d18e",
   "metadata": {},
   "source": [
    "It is evident that the churn dataset is highly imbalanced, with 85% of the dataset not churned while a decent 14% of the dataset being churned. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f3cb9-f827-4fcf-92e2-a634987151d6",
   "metadata": {},
   "source": [
    "#### 2.2.2 Visualising the distributions of the numerical features\n",
    "\n",
    "Most of the features tend to form a normal distribution, so aren't majorly right skewed or left skewed. However, number_service_calls, total international calls, number_vmail_messages are more right skewed than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeecbc8-35aa-4038-abe2-3a5198ce4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.hist(figsize = (20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a063a9e4-12cc-48b6-97e7-4a7444aaf566",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.3 Correlation matrix \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd4eed-6854-46f1-b7f7-6a8dd1695fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train_one_hot.corr()\n",
    "\n",
    "top_features_corr = corr['churn'].sort_values(ascending = False).head(10)\n",
    "top_features = top_features_corr.index.values\n",
    "top_features_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f09f0c-7489-413e-b3db-69117bb0ec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_features_corr = corr['churn'].sort_values(ascending = True).head(10)\n",
    "bot_features = list(bot_features_corr.index.values)\n",
    "bot_features.append('churn')\n",
    "bot_features_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd57e677-bf0b-46dc-8343-396d4a1521d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heatmap(train):\n",
    "    correlations = train.corr()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f',\n",
    "                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70})\n",
    "    plt.show()\n",
    "    \n",
    "correlation_heatmap(train_one_hot[top_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5e9e08-0298-423d-877d-86813ace195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_heatmap(train_one_hot[bot_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4c83f9-23e3-4a4d-816c-1d13fa3fe240",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.3 Plots vs churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061d6ac-d8d7-4865-83b1-d0e3651d2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=train, x=\"international_plan\", hue=\"churn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4015296-506e-48bd-aec1-88deb09429cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=train[\"churn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d06dd1-2236-4473-bbe7-c0efe835ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=train, x=\"total_charge\", hue=\"churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8030da2-6416-45be-ba3d-341722c055f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=train, x=\"total_minutes\", hue=\"churn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99694e7b-890a-4a42-8d0f-6bdf7bbf0499",
   "metadata": {},
   "source": [
    "### 2.2.4 Plot the scatterplots - maybe consider plotting a pair plot as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a629a9d0-a928-4db8-a6c3-c5b310be2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.plot(kind='scatter', x='total_charge', y='total_day_minutes', alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a1d5a-903f-4d21-bcee-74366e929536",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.3 Preparing the inputs and choosing suitable features \n",
    "\n",
    "In order to pick the best features, i'll create a few datasets with subsets of features and then train a model using 5 fold cross validation to find \n",
    "the best dataset to use for hyperparameter tuning.\n",
    "\n",
    "These are the datasets that will be created: \n",
    "\n",
    "- Full dataset\n",
    "- Top 10 features that correlated with churn\n",
    "- Recursive feature elimination (RFE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99727c35-5c1e-4744-bd58-938f9c5774c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "voters = [('lgbm', LGBMClassifier(verbosity = -1, num_threads = 8)),\n",
    "          ('rf', RandomForestClassifier()),\n",
    "          ('xgb', XGBClassifier(verbosity = 0)),\n",
    "          ('cat', CatBoostClassifier(verbose=True))]\n",
    "        \n",
    "\n",
    "models = {'logreg': LogisticRegression(),\n",
    "          'lgbm': LGBMClassifier(verbosity = -1, num_threads = 8), \n",
    "          'xgb': XGBClassifier(verbosity = 0), \n",
    "          'rf': RandomForestClassifier(n_jobs = -1),\n",
    "          'cat': CatBoostClassifier(verbose=True),\n",
    "          'vc:': VotingClassifier(estimators=voters, voting='soft')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c86d50c-64dd-4905-ab06-8ec445e741b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# catboost, xgboost, logistic regression, lgbm, mlp classifier, random forest\n",
    "\n",
    "def evaluate_rfe(RFEClassifier, X_train, y_train): \n",
    "    \"\"\"\n",
    "   # Evaluates the recursive feature elimination performance for a given model using the LGBM classifier\n",
    "   # :returns: cross_validation scores\n",
    "    \"\"\"\n",
    "    # create pipeline\n",
    "    rfe = RFECV(estimator=RFEClassifier)\n",
    "    model = LGBMClassifier()\n",
    "    pipeline = Pipeline(steps=[('scaler', StandardScaler()), ('feature_selection',rfe),('model',model)])\n",
    "    # evaluate model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "    n_scores = cross_val_score(pipeline, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return n_scores\n",
    "\n",
    "scores, names = list(), list()\n",
    "eval_rfe_results = {}\n",
    "\n",
    "for model in models: \n",
    "    print(model)\n",
    "    score = evaluate_rfe(models[model], X_train, y_train)\n",
    "    scores.append(score)\n",
    "    names.append(model)\n",
    "    eval_rfe_results [model] = score\n",
    "    print('>%s %.3f (%.3f)' % (model, mean(score), std(score)))\n",
    "\n",
    "results = pd.DataFrame(eval_rfe_results)\n",
    "results.to_csv('results/eval_rfe_results.csv', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69add9a3-a447-4f70-96e7-e1f5221457ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box_plot(X, labels, xlabel, ylabel, title, filename, figsize, t):\n",
    "    \"\"\"\n",
    "    Plots a boxplot of values while annotating the mean and standard deviation\n",
    "    :X: The input data\n",
    "    :labels: column labels of the input data\n",
    "    :xlabel: x axis label\n",
    "    :ylabel: y axis label\n",
    "    :filename: Name of the figure file\n",
    "    :figsize: size of the boxplot\n",
    "    :t: vector of fit times\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plot boxpot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    bp = plt.boxplot(X, labels=labels, showmeans=True) \n",
    "    \n",
    "    # Annotate the boxplot with mean and std\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    for i, line in enumerate(bp['medians']):\n",
    "        x, y = line.get_xydata()[1]\n",
    "        text = ' μ={:.3f}\\n σ={:.3f}\\n t={:.2f}'.format(mean[i], std[i],t[i])\n",
    "        ax.annotate(text, xy=(x, y))\n",
    "    \n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6b441-c14e-4936-9a7b-d55876b5a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('results/eval_rfe_results.csv')\n",
    "\n",
    "plot_box_plot(X=results_df.values,  \n",
    "              labels=results_df.columns, \n",
    "              xlabel = 'Models', \n",
    "              ylabel = 'AUC Score',\n",
    "              title = 'Recursive feature elimination performance across 4 models',\n",
    "              filename = 'images/eval_rfe_results.png',\n",
    "              figsize = (10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501a914c-b572-41c5-87d0-21234eda3e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rfe (X_train, y_train, classifier):\n",
    "    \"\"\"\n",
    "    Implements recursive feature elimination with 5 fold cross validation\n",
    "    :X_train: \n",
    "    :y_train: \n",
    "    :return: transformed dataframe (X_train_rfe), features selected\n",
    "    \"\"\"\n",
    "    estimator = classifier\n",
    "    selector = RFECV(estimator, step=3, cv=5, scoring = 'roc_auc')\n",
    "    selector = selector.fit(X_train, y_train)\n",
    "    X_train_rfe = selector.transform(X_train) # transform the training set\n",
    "    X_val_rfe = selector.transform(X_val)  # transform the validation set with the training fit\n",
    "    X_test_rfe = selector.transform(X_test) # transform the test set with the training fit\n",
    "    \n",
    "    features_selected = {}\n",
    "    \n",
    "    for i in range(len(selector.support_)):\n",
    "        if selector.support_[i] == True:\n",
    "            features_selected[i] = selector.feature_names_in_[i]\n",
    "        \n",
    "    return X_train_rfe,  X_val_rfe, X_test_rfe, features_selected\n",
    "\n",
    "X_train_rfe, X_val_rfe, X_test_rfe, features_selected = rfe(X_train, y_train, models['xgb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211fa31d-da29-450a-a686-e49a1899c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5970c1f9-1562-49cc-a239-2f8e6ab6782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove churn from top features array as it is the target variables\n",
    "churn_index = 0\n",
    "top_features = np.delete(top_features, churn_index)\n",
    "\n",
    "X_train_list = {'full_dataset': X_train, \n",
    "                'top_10_features': X_train[top_features], \n",
    "                'RFE' : X_train_rfe}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78defba5-fe69-43a0-bac3-01c0c65aecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open('X_train_list.pickle', 'wb') as f:\n",
    "    pickle.dump(X_train_list, f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f652947e-bafe-443e-83d2-0f75e5ddce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train_list.pickle', 'rb') as f:\n",
    "    X_train_datasets = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47dc860-d423-483d-b745-dad2fcf6cbec",
   "metadata": {},
   "source": [
    "## 2.4 Train the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad0b3e-b772-47ed-bba6-718a25750018",
   "metadata": {},
   "source": [
    "### 2.4.1 Find top peforming dataset\n",
    "\n",
    "Here we evaluate the top peforming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c4ecb1-c3b8-477b-bd0a-92dd379e4c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(model, X_train, y_train): \n",
    "    \"\"\"\n",
    "    Evaluates trainiing performance with repeated stratified cross validation of a given dataset\n",
    "    :returns: cross_validation scores\n",
    "    \"\"\"\n",
    "    # evaluate model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    n_scores = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return n_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943d2e9-0038-4a55-8ba3-b097a857ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, names = list(), list()\n",
    "eval_dataset_results = {}\n",
    "\n",
    "for X_train in X_train_list: \n",
    "    print(X_train)\n",
    "    score = evaluate_dataset(models['lgbm'], X_train_list[X_train], y_train)\n",
    "    scores.append(score)\n",
    "    names.append(X_train)\n",
    "    eval_dataset_results [X_train] = score\n",
    "    print('>%s %.3f (%.3f)' % (X_train, score.mean(), score.std()))\n",
    "\n",
    "results = pd.DataFrame(eval_dataset_results)\n",
    "results.to_csv('results/eval_dataset_results.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00ff33-68cf-4517-a941-25322effb3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('results/eval_dataset_results.csv')\n",
    "\n",
    "plot_box_plot(X=results_df.values,  \n",
    "              labels=results_df.columns, \n",
    "              xlabel = 'Datasets', \n",
    "              ylabel = 'AUC Score',\n",
    "              title = 'Training  performance across the dataset with 5 fold cross validation',\n",
    "              filename = 'images/eval_dataset_results.png',\n",
    "              figsize = (10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c3f424-f004-4d8b-924f-dc0cee5352bc",
   "metadata": {},
   "source": [
    "### 2.4.2 Find top peforming models\n",
    "\n",
    "In this section we compare the performance of the 5 models with their default parameters with 10 fold cross validation on the Rfe dataset: \n",
    "- Logistic Regression\n",
    "- Catboost \n",
    "- XgBoost \n",
    "- random forest\n",
    "- Ligthgbm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c5fd0-93b3-4c8f-8f7f-e45503b452a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_models(models, X, y , csv_name):\n",
    "    \"\"\"\n",
    "    Model list setup for evaluation\n",
    "    :X_train: type of dataset\n",
    "    :models: Dictionary of models\n",
    "    :filenme: filename of the results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up data storage\n",
    "    scores, names, times  = list(), list(), list()\n",
    "    eval_dataset_results = {}\n",
    "\n",
    "    for model in models: \n",
    "        # Run the evaluation\n",
    "        start = timeit.default_timer()\n",
    "        print(f'Evaluation has started on {model}')\n",
    "        score = cross_validate(models[model], X, y)\n",
    "        \n",
    "        # store values\n",
    "        scores.append(score)\n",
    "        names.append(X_train)\n",
    "        eval_dataset_results[model] = score\n",
    "        end = timeit.default_timer()\n",
    "        t = end - start\n",
    "        times.append(t)\n",
    "\n",
    "    # Store the final results\n",
    "    results = pd.DataFrame(eval_dataset_results)\n",
    "    results.to_csv(csv_name, index=False, header=True)\n",
    "    \n",
    "    return results, times\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cacbb1-bb63-4292-bdfb-60ac4839314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, times = evaluate_models(models=models, \n",
    "                                       X= X_train_list['RFE'], \n",
    "                                       y = y_train\n",
    "                                       csv_name = 'results/eval_model_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a16300-8033-4b65-a5dc-9fe09a5f5f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_plot(X=results.values,  \n",
    "              labels=results.columns, \n",
    "              xlabel = 'Datasets', \n",
    "              ylabel = 'AUC Score',\n",
    "              title = 'Training  performance across the dataset with 5 fold cross validation',\n",
    "              filename = 'images/eval_model_results.png',\n",
    "              figsize = (15,8),\n",
    "              t=times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c0be60-526d-444e-a040-f1ee212b50f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.4.3 Hyperparameter tuning of top models\n",
    "\n",
    "#### Bayesian Optimisation\n",
    "\n",
    "Bayesian optimization works by constructing a posterior distribution of functions (gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not, as seen in the picture below.\n",
    "\n",
    "![bayesian_opt](images/bo_example.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d47f44-bc86-495f-941b-5121acb7d142",
   "metadata": {},
   "source": [
    "First we create a wrapper function to report the results of each of the hyperparameter sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ed7e2-f9e9-4599-8dca-e9c74544b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_perf(optimizer, X, y, model_name,  csv_name, callbacks=None):\n",
    "    \"\"\"\n",
    "    A wrapper for measuring time and performances of different optmizers\n",
    "    \n",
    "    optimizer = a sklearn or a skopt optimizer\n",
    "    X = the training set \n",
    "    y = our target\n",
    "    title = a string label for the experiment\n",
    "    \"\"\"\n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    if callbacks is not None:\n",
    "        optimizer.fit(X, y, callback=callbacks)\n",
    "    else:\n",
    "        optimizer.fit(X, y)\n",
    "        \n",
    "    d=pd.DataFrame(optimizer.cv_results_)\n",
    "    d.to_csv(csv_name, index=False, header=True)\n",
    "    best_score = optimizer.best_score_\n",
    "    best_score_std = d.iloc[optimizer.best_index_].std_test_score\n",
    "    best_params = optimizer.best_params_\n",
    "    duration = timeit.default_timer() - start\n",
    "    print((model_name + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n",
    "           + u\"\\u00B1\"+\" %.3f\") % (duration, \n",
    "                                   len(optimizer.cv_results_['params']),\n",
    "                                   best_score,\n",
    "                                   best_score_std))    \n",
    "    print('Best parameters:')\n",
    "    pprint.pprint(best_params)\n",
    "    print()\n",
    "    return best_params, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e8351-91ce-4022-9144-9c8021b5fbc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyper parameters spaces of each of the models\n",
    "\n",
    "# Setting the search space\n",
    "lgbm_search_spaces = {\n",
    "    'learning_rate': Real(0.01, 1.0, 'log-uniform'),     # Boosting learning rate\n",
    "    'n_estimators': Integer(30, 5000),                   # Number of boosted trees to fit\n",
    "    'num_leaves': Integer(2, 512),                       # Maximum tree leaves for base learners\n",
    "    'max_depth': Integer(-1, 256),                       # Maximum tree depth for base learners, <=0 means no limit\n",
    "    'subsample': Real(0.01, 1.0, 'uniform'),             # Subsample ratio of the training instance\n",
    "    'subsample_freq': Integer(1, 10),                    # Frequency of subsample, <=0 means no enable\n",
    "    'colsample_bytree': Real(0.01, 1.0, 'uniform'),      # Subsample ratio of columns when constructing each tree\n",
    "    'reg_lambda': Real(1e-9, 100.0, 'log-uniform'),      # L2 regularization\n",
    "    'reg_alpha': Real(1e-9, 100.0, 'log-uniform'),       # L1 regularization\n",
    "   }\n",
    "\n",
    "rf_search_spaces = {\n",
    "    'bootstrap': Categorical([True, False]),             # Method of selecting samples for training each tree\n",
    "    'max_depth': Integer(1, 200),                        # Maximum number of levels in tree\n",
    "    'max_features': Categorical(['auto', 'sqrt']),       # Number of features to consider at every split\n",
    "    'min_samples_leaf': Integer(1, 5),                   # Minimum number of samples required at each leaf node\n",
    "    'min_samples_split': Integer(2, 10),                 # Minimum number of samples required to split a node\n",
    "    'n_estimators': Integer(200, 2000)}                  # Number of trees in the random forest\n",
    "    \n",
    "xgb_search_spaces = {\n",
    "    'learning_rate': Real(0.01, 1.0, 'uniform'),\n",
    "     'max_depth': Integer(2, 12),\n",
    "     'subsample': Real(0.1, 1.0, 'uniform'),\n",
    "     'colsample_bytree': Real(0.1, 1.0, 'uniform'), # subsample ratio of columns by tree\n",
    "     'reg_lambda': Real(1e-9, 100., 'uniform'), # L2 regularization\n",
    "     'reg_alpha': Real(1e-9, 100., 'uniform'), # L1 regularization\n",
    "     'n_estimators': Integer(50, 5000)}\n",
    "\n",
    "cat_search_spaces = {\n",
    "    'iterations': Integer(10, 2000),\n",
    "    'depth': Integer(1, 12),\n",
    "    'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "    'random_strength': Real(1e-9, 10, 'log-uniform'), # randomness for scoring splits\n",
    "    'bagging_temperature': Real(0.0, 1.0), # settings of the Bayesian bootstrap\n",
    "    'l2_leaf_reg': Integer(2, 100), # L2 regularization\n",
    "   }\n",
    "\n",
    "models_search_spaces = {\n",
    "    'lgbm': [LGBMClassifier(verbosity = -1, num_threads = 8, random_state=0, objective ='binary'), lgbm_search_spaces], \n",
    "    'xgb': [XGBClassifier(verbosity = 0, random_state = 0), xgb_search_spaces], \n",
    "    'rf': [RandomForestClassifier(n_jobs = -1, random_state=0), rf_search_spaces],\n",
    "    'cat': [CatBoostClassifier(verbose=False, random_state=0), cat_search_spaces]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c576231f-7074-42d5-bf60-1d84dacf09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_search (model, search_spaces, model_name, X_train, y_train): \n",
    "    \n",
    "    \n",
    "    # Wrapping everything up into the Bayesian optimizer\n",
    "    opt = BayesSearchCV(estimator=model,                                    \n",
    "                    search_spaces=search_spaces,                      \n",
    "                    scoring='roc_auc',                           \n",
    "                    cv=5,                                           \n",
    "                    n_iter=60,                                        # max number of trials\n",
    "                    n_points=3,                                       # number of hyperparameter sets evaluated at the same time\n",
    "                    n_jobs=-1,                                        # number of jobs\n",
    "                    iid=False,                                        # if not iid it optimizes on the cv score\n",
    "                    return_train_score=False,                         \n",
    "                    refit=False,                                      \n",
    "                    optimizer_kwargs={'base_estimator': 'GP'},        # optmizer parameters: we use Gaussian Process (GP)\n",
    "                    random_state=0)                                   # random state for replicability\n",
    "\n",
    "    overdone_control = DeltaYStopper(delta=0.0001)               # We stop if the gain of the optimization becomes too small\n",
    "    time_limit_control = DeadlineStopper(total_time=60 * 60 * 7) # We impose a time limit (6 hours)\n",
    "\n",
    "    best_params, duration = report_perf(optimizer = opt, \n",
    "                              X = X_train, \n",
    "                              y = y_train, \n",
    "                              model_name = model_name, \n",
    "                              csv_name = f'hyperopt_results_{model_name}.csv', \n",
    "                              callbacks=[overdone_control, time_limit_control])\n",
    "    return [best_params, duration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76fc663-9b7f-4e49-a1e7-d05bc3803efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperopt = {}\n",
    "\n",
    "for model_name in models_search_spaces:\n",
    "    hyperopt[model_name] =  bayesian_search (model = models_search_spaces[model_name][0], \n",
    "                                             search_spaces = models_search_spaces[model_name][1],\n",
    "                                             model_name = model_name, \n",
    "                                             X_train = X_train_datasets['RFE'], \n",
    "                                             y_train  = y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af04565d-06d7-4d98-8b6d-f1baa15b26cc",
   "metadata": {},
   "source": [
    "### 3. Evaluate on validation set and submit to kaggle\n",
    "\n",
    "After hyperparameter tuning, I will instantiate the the models with their newly tuned parameters to then evaluate on the validation set. \n",
    "\n",
    "Evaluate the model based on other metrics as well\n",
    "- Confusion matrix \n",
    "- Accuracy, f1 score, recall and precision\n",
    "- AUC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e83384f-da06-4388-82b8-d6ea15258122",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_models = {\n",
    "    'lgbm': LGBMClassifier(verbosity = -1, num_threads = 8, random_state=0, objective ='binary', **hyperopt['lgbm'][0]), \n",
    "    'xgb':  XGBClassifier(verbosity = 0, random_state = 0, **hyperopt['xgb'][0]), \n",
    "    'rf': RandomForestClassifier(n_jobs = -1, random_state=0, **hyperopt['rf'][0]),\n",
    "    'cat': CatBoostClassifier(verbose=False, random_state=0, **hyperopt['cat'][0])}\n",
    "vclf = VotingClassifier(n_jobs =-1, estimators=[('lgbm', tuned_models['lgbm']), \n",
    "                                              ('xgb', tuned_models['xgb']), \n",
    "                                              ('rf', tuned_models['rf']),\n",
    "                                              ('cat', tuned_models['cat'])],\n",
    "                                               voting='soft')\n",
    "tuned_models['vclf'] = vclf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11cf917-3b81-4259-834c-b3a6f87d1068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the methods ready \n",
    "\n",
    "def save_object(obj, file_name):\n",
    "    \"\"\"\n",
    "    Saves the chosen onbject\n",
    "    :obj: chosen object\n",
    "    :file_name: name of file\n",
    "    \"\"\"\n",
    "    with open(f'{file_name}.pickle', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "        \n",
    "def plot_confusion_matrix(clf, clf_name, X_val, y_val): \n",
    "    ConfusionMatrixDisplay.from_estimator(clf, X_val, y_val)\n",
    "    plt.show()\n",
    "    plt.savefig(f'{clf_name} confusion matrix on the validation set.png')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def plot_roc_curve(models, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Plots the receiving operating characteristic curve. \n",
    "    Corresponds to Figure 7 in the report\n",
    "    \"\"\" \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.xlim(0, 0.2)\n",
    "    plt.ylim(0.8, 1)\n",
    "    for model_name in models:\n",
    "        model = models[model_name]\n",
    "        y_pred_proba = model.predict_proba(X_val)\n",
    "        fpr, tpr, thresholds = roc_curve(y_val_B, y_pred_proba)\n",
    "        plt.plot(fpr, tpr, linewidth=2, label=key)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16) \n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    \n",
    "    plt.grid(True)\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curve(models, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Plots precision vs recall. \n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.xlim(0, 0.2)\n",
    "    plt.ylim(0.8, 1)\n",
    "    for key in results:\n",
    "        model = results[key][0]\n",
    "        y_pred_proba = model.predict_proba(X_val)\n",
    "        precisions, recalls, thresholds = precision_recall_curve(y_val_B, y_pred_proba)\n",
    "        plt.plot(precisions, recalls, linewidth=2, label=key)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('Recall', fontsize=16) \n",
    "    plt.ylabel('Precision', fontsize=16)    \n",
    "    plt.grid(True)\n",
    "    plt.title('Precision Recall curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f14ad-97e0-4774-bfc3-6b054dc101bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.2 Confusion Matrices \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54043e73-99b4-494d-9d4d-96ef8061c2ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tuned_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23544\\442628797.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuned_models\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuned_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'RFE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_rfe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtarget_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tuned_models' is not defined"
     ]
    }
   ],
   "source": [
    "for model_name in tuned_models:\n",
    "    model = tuned_models[model_name]\n",
    "    model.fit(X_train_datasets['RFE'], y_train)\n",
    "    y_pred = model.predict(X_val_rfe)\n",
    "    target_names = [0,1]\n",
    "                  \n",
    "    # Get results        \n",
    "    plot_confusion_matrix(model, model_name, X_val_rfe, y_val)\n",
    "    print(classification_report(y_val, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee186e7-c297-4839-951b-3c6000246af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(tuned_models, 'tuned_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e367b-5ac9-4910-9d7f-92f724426d05",
   "metadata": {},
   "source": [
    "### 4. Deploy model to sagemaker endpoint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
